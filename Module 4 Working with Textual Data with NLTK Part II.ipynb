{"cells":[{"cell_type":"markdown","id":"4b38feb7","metadata":{"id":"4b38feb7"},"source":["## Module 4: Working with Textual Data with NLTK Part II (10 Points).\n","\n","\n","In this tutorial, we will:\n","1. Explore the development of word2vec embedding of word tokens.\n","2. A shallow taste of the PyTorch framework for deep learning.\n","\n","\n","**Note: There is an associated submission for this exercise.**"]},{"cell_type":"markdown","id":"735b85e1","metadata":{"id":"735b85e1"},"source":["Let's still start with the `examp_doc` we have used in last tutorial. Alternatively, you can replace it with other paragraphs."]},{"cell_type":"code","execution_count":1,"id":"a6c84425","metadata":{"id":"a6c84425","executionInfo":{"status":"ok","timestamp":1761204879883,"user_tz":-480,"elapsed":4591,"user":{"displayName":"Trường Lợi Lý","userId":"01168430367908089140"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":2,"id":"b3517845","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3517845","executionInfo":{"status":"ok","timestamp":1761204884510,"user_tz":-480,"elapsed":13,"user":{"displayName":"Trường Lợi Lý","userId":"01168430367908089140"}},"outputId":"25f666fc-78b3-41c2-b89c-543efae85b0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Growing use of the Internet and social media in the past decade has led to an explosion in the amount of \n","social and behavioral data available to researchers. This in turn has created huge opportunities for social scientists to \n","study human behavior and social interaction in unprecedented detail. Leveraging these opportunities requires collaborative, \n","interdisciplinary efforts involving computer and information scientists, physicists, and mathematicians who know how to \n","build the telescope and economists, political scientists, and sociologists who know where to aim it. Computational social \n","science exists at the intersection of these varied disciplines.\n"]}],"source":["# Trick: use triple quotes (\"\"\" ...... \"\"\") to define multi-line texts in Python.\n","\n","examp_doc = \"\"\"Growing use of the Internet and social media in the past decade has led to an explosion in the amount of\n","social and behavioral data available to researchers. This in turn has created huge opportunities for social scientists to\n","study human behavior and social interaction in unprecedented detail. Leveraging these opportunities requires collaborative,\n","interdisciplinary efforts involving computer and information scientists, physicists, and mathematicians who know how to\n","build the telescope and economists, political scientists, and sociologists who know where to aim it. Computational social\n","science exists at the intersection of these varied disciplines.\"\"\"\n","\n","\n","print(examp_doc)"]},{"cell_type":"code","execution_count":7,"id":"f7d8ecb7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7d8ecb7","executionInfo":{"status":"ok","timestamp":1761204986813,"user_tz":-480,"elapsed":12,"user":{"displayName":"Trường Lợi Lý","userId":"01168430367908089140"}},"outputId":"23510164-ec59-4384-c734-6c522f89feb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['grow', 'use', 'internet', 'social', 'media', 'past', 'decad', 'led', 'explos', 'amount', 'social', 'behavior', 'data', 'avail', 'research'], ['turn', 'creat', 'huge', 'opportun', 'social', 'scientist', 'studi', 'human', 'behavior', 'social', 'interact', 'unpreced', 'detail'], ['leverag', 'opportun', 'requir', 'collabor', 'interdisciplinari', 'effort', 'involv', 'comput', 'inform', 'scientist', 'physicist', 'mathematician', 'know', 'build', 'telescop', 'economist', 'polit', 'scientist', 'sociologist', 'know', 'aim'], ['comput', 'social', 'scienc', 'exist', 'intersect', 'vari', 'disciplin']]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import string\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stemmer = nltk.stem.PorterStemmer()\n","stopword_lst = nltk.corpus.stopwords.words('english')\n","punct_lst = list('''!()-[]{};:'\"\\\\,<.>/?@#$%^&*_~''') # Converted the string to a list of characters\n","\n","def clean_and_tokenize(examp_doc):\n","    # write down the corresponding procedures.\n","    # the six lines correspond to six steps.\n","\n","    out = nltk.sent_tokenize(examp_doc)\n","    out = [nltk.word_tokenize(sent) for sent in out]\n","    out = [[word.lower() for word in sent] for sent in out]\n","    out = [[word for word in sent if word not in stopword_lst] for sent in out]\n","    out = [[word for word in sent if word not in punct_lst] for sent in out]\n","    out = [[stemmer.stem(word) for word in sent] for sent in out]\n","\n","    return out\n","\n","\n","token_lsts = clean_and_tokenize(examp_doc)\n","print(token_lsts)"]},{"cell_type":"markdown","id":"452a0361","metadata":{"id":"452a0361"},"source":["#### Tokenize the document.\n","\n","**Exercise 1** Use below code cell to convert the document into a list of tokens. Each token should:\n","- split the paragraph into sentences\n","- for each sentence, further split them into individual words.\n","- turn all words into lower cases (i.e., \"a\" insteand of \"A\")\n","- remove stop words.\n","- remove punctuations.\n","- stem to the root.\n","\n","You can add a cell box below and test the code line by line before writing the function."]},{"cell_type":"markdown","id":"31d1d835","metadata":{"id":"31d1d835"},"source":["### 1. Bag-of-Word (BoW) Encoding\n","\n","This section will experiment the creation of BoW with the provided \"examp_doc\".\n","\n","   1. The *first* step is to clean and prepare the data, which we have already done with the function `clean_and_tokenize()`, and the output is `token_lsts`.\n","\n","   2. Then the *second* step is to create a vocabulary, consisting all unique words in the document. Please check and run the below code to achieve this step."]},{"cell_type":"code","execution_count":null,"id":"9e209e4b","metadata":{"scrolled":true,"id":"9e209e4b"},"outputs":[],"source":["vocab = sorted(list(set([token for token_lst in token_lsts for token in token_lst])))"]},{"cell_type":"code","execution_count":null,"id":"1350bc32","metadata":{"id":"1350bc32"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"c9c4ed09","metadata":{"id":"c9c4ed09"},"source":["<span style='background-color: #FFFF00;'> **Question:** what is the dimension for the one-hot word encoding derived from this vocabulary? (**1 Point**) </span>\n","\n","\n","**Answer:** (double click and type your answers here)"]},{"cell_type":"markdown","id":"7d505b64","metadata":{"id":"7d505b64"},"source":["3. The *third* step is to convert each documents (i.e., each sentence in `exemp_doc` or each list in `token_lsts`) into the BoW encoding. Write the function in the below code cell to achieve this."]},{"cell_type":"code","execution_count":null,"id":"0d47f49a","metadata":{"id":"0d47f49a"},"outputs":[],"source":["## finish the code in this step.\n","import numpy as np\n","\n","def tokenLst_to_BOW(token_lsts, vocab):\n","    # try finish this function by your self.\n","\n","    out =\n","\n","    return out\n","\n","\n","bow_encoding = tokenLst_to_BOW(token_lsts, vocab)\n","\n","print(bow_encoding[0])"]},{"cell_type":"markdown","id":"3360e6ef","metadata":{"id":"3360e6ef"},"source":["Bag of Words (BOW) encoding can be utilized to analyze the similarity among texts by comparing the frequency distribution of various words. Therefore, BOW encoding can serve as input for NLP tasks that inherently rely on similarity scores, such as document classification (e.g., determining the category of news articles), topic modeling, and latent semantic analysis.\n","\n","Below just showed a simple example for comparing similarity of the four sentences in the `exemp_doc` with the [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) function in sklearn. Based on the results, it can be inferred that the first and second sentences are the most alike, with a cosine similarity score of 0.31."]},{"cell_type":"code","execution_count":null,"id":"d4cd8de1","metadata":{"id":"d4cd8de1","outputId":"8d6ae7c6-7272-4ef5-9f0f-3da6446c2b6e"},"outputs":[{"data":{"text/plain":["array([[1.        , 0.31311215],\n","       [0.31311215, 1.        ]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","print(cosine_similarity((bow_encoding[0], bow_encoding[1])))"]},{"cell_type":"markdown","id":"c1471255","metadata":{"id":"c1471255"},"source":["<span style='background-color: #FFFF00;'> **Exercise:** In below code cell, write the function to convert BoW encoding (a list of lists) into TF-IDF encoding. (**3 Points**) </span>\n","- The input is BOW encoding.\n","- The output is tf-idf encoding.\n","\n","<div>\n","<img src=\"attachment:image-2.png\" align=\"center\" width=\"400\">\n","<img src=\"attachment:image-3.png\" align=\"center\" width=\"300\">\n","<img src=\"attachment:image.png\" align=\"center\" width=\"300\">\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"91a84004","metadata":{"id":"91a84004"},"outputs":[],"source":["def bow_to_tfidf(bow_encoding):\n","    # write your code below\n","\n","\n","\n","\n","    return tfidf_encoding\n","\n","\n","bow_to_tfidf(bow_encoding)"]},{"cell_type":"markdown","id":"f5c72d6b","metadata":{"id":"f5c72d6b"},"source":["### 2. Word2Vec Embedding.\n","\n","[`PyTorch`](https://pytorch.org/), developed by Facebook's Artificial Intelligence Research group (FAIR), is a powerful library for building deep learning networks. It provides implementations of many commonly-used network layers. The process of building a deep learning network is akin to building with blocks (搭积木). When building with blocks, you arrange each block to create an overall appealing structure. Similarly, when building a deep learning model, you arrange the different layers to optimize its efficiency.\n","\n","![image-3.png](attachment:image-3.png)\n","\n","We will use PyTorch to implement the Continuous Bag-of-Words (CBOW) model we have learnt in the lecture session. To start, we need to prepare the original file `exemp_doc` into bag-of-words. You can run the below code cell to prepare the data.\n","\n","Please also be noted that the CBOW embedding and BOW encoding (or one-hot encoding) are two different things."]},{"cell_type":"code","execution_count":null,"id":"4339f82e","metadata":{"id":"4339f82e","outputId":"7b9bd494-d8dc-49aa-a291-9b1d829e6d59"},"outputs":[{"name":"stdout","output_type":"stream","text":["(('grow', 'use', 'social', 'media'), 'internet')\n"]}],"source":["context_size = 2\n","\n","data = []\n","\n","for sent in token_lsts:\n","    for i in range(context_size, len(sent)-2):\n","        context = (sent[i-2], sent[i-1], sent[i+1], sent[i+2])\n","        target = sent[i]\n","        data.append((context, target))\n","\n","\n","print(data[0])"]},{"cell_type":"markdown","id":"5a8e146a","metadata":{"id":"5a8e146a"},"source":["CBOW predicts the word with its contextual words. For example, predicting \"*internet*\" based on the list of words: [\"*grow*\", \"*use*\", \"*social*\", \"*media*\"].\n","\n","\n","**A shallow taste of deep learning**\n","\n","In our lecture session, we have explained the principle of Word2Vec embeddings. Below code cell presents a simple deep learning model, written with PyTorch, for Continuous Bag-of-Words (CBOW). The corresponding network structure is also provided in Figure 1.\n","\n","\n","<div>\n","<img src=\"attachment:image-2.png\" align=\"center\" width=\"700\">\n","</div>\n","\n","**Figure 1**. The network structure of CBOW.\n","\n","Some online documents in PyTorch you may find helpful:\n","- [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","- [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","- [`F.log_softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)"]},{"cell_type":"code","execution_count":null,"id":"e209f833","metadata":{"id":"e209f833"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F   # load necessary packages & functions from PyTorch\n","\n","\n","class CBOW(nn.Module): # define the class \"CBOW\", which is a child class derived from the parent class: \"nn.Module\"\n","\n","    def __init__(self, vocab_size, embedding_dim=5, hidd_dim = 16, context_size = 2):\n","        super(CBOW, self).__init__() # the \"CBOW\" class inherits all properties and methods from its parent class.\n","\n","        # initialize the embedding layer.\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Define the first fully connected layer (linear transformation) which takes the concatenated embeddings as input\n","        self.linear1 = nn.Linear(context_size * embedding_dim, hidd_dim)\n","\n","        # Define the second fully connected layer which outputs scores for each word in the vocabulary\n","        self.linear2 = nn.Linear(hidd_dim, vocab_size)\n","\n","    def forward(self, inputs): # Define the forward pass of the network\n","        embeds = self.embeddings(inputs) # Get the embeddings for the input words\n","\n","        # Flatten the embeddings (from context_size x embedding_dim to a single vector)\n","        out = embeds.view((1, -1))\n","\n","        # Pass the flattened embeddings through the first linear layer\n","        out = self.linear1(out)\n","\n","        # Pass the output of the first linear layer through the second linear layer\n","        out = self.linear2(out)\n","\n","        # Apply log softmax to get log probabilities for each word in the vocabulary\n","        log_probs = F.log_softmax(out, dim=1)\n","\n","        return(log_probs) # Return the log probabilities"]},{"cell_type":"markdown","id":"f8ae7eb4","metadata":{"id":"f8ae7eb4"},"source":["<span style='background-color: #FFFF00;'> Please carefully review Figure 1 and the 13-line PyTorch code. Analyze the code and answer the following questions (**4 Points**).</span>\n","\n","---\n","**Q1:** which line of the code correspond to the \"Embedding Layer\" on the figure, and what is the dimension of inputs and outputs for that layer?\n","\n","**A1:**\n","\n","---\n","\n","**Q2:** which line of the code correspond to the \"1st Linear Layer\" on the figure, and what is the dimension of inputs and outputs for that layer?\n","\n","**A2:**\n","\n","---\n","**Q3:** which line of the code correspond to the \"2nd Linear Layer\" on the figure, and what is the dimension of inputs and outputs for that layer?\n","\n","**A3:**\n","\n","---\n","**Q4:** what is the purpose of the softmax?\n","\n","**A4:**\n","\n","---"]},{"cell_type":"markdown","id":"d89993d2","metadata":{"id":"d89993d2"},"source":["Run below code cell to train the model and check the loss curve."]},{"cell_type":"code","execution_count":null,"id":"c29c5043","metadata":{"id":"c29c5043","outputId":"03750b38-16d6-4969-f9dd-91699e4a9a3f"},"outputs":[{"data":{"text/plain":["array([[-0.10768455]], dtype=float32)"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["cosine_similarity(model.embeddings(torch.tensor([0])).detach().numpy(),\n","                  model.embeddings(torch.tensor([1])).detach().numpy())"]},{"cell_type":"code","execution_count":null,"id":"c368136e","metadata":{"id":"c368136e"},"outputs":[],"source":["# step 1: embedding\n","out = model.embeddings(context_ids)\n","\n","# 2. faltten\n","out = out.view((1, -1))\n","\n","# 3. first linear layer:\n","\n","out = model.linear1(out)\n","\n","# 4. second linear layer:\n","out = model.linear2(out)\n","\n","# 5. softmax\n","\n","prob = F.softmax(out).detach().numpy()[0] # output the probability of the target word.\n","\n","plt.figure(figsize = (10, 5))\n","plt.bar(vocab, prob)\n","plt.xticks(rotation = 90)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"8592143b","metadata":{"id":"8592143b"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","# Create a dictionary mapping each word to its index in the vocabulary, as nn.Embedding layer only takes indices.\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","losses = []\n","loss_function = nn.NLLLoss() # Define the loss function to be used (Negative Log Likelihood Loss)\n","model = CBOW(len(vocab), embedding_dim=5, context_size=4) # Initialize the CBOW model with necessary inputs.\n","optimizer = optim.SGD(model.parameters(), lr=0.05)  # Define the optimizer and learning rate to update the model's parameters.\n","\n","\n","for epoch in range(20):\n","    total_loss = 0 # Initialize the total loss for the current epoch\n","    for context, target in data: # Iterate over each context-target pair in the dataset\n","        context_ids = make_context_vector(context, word_to_ix) # Convert the context words to their corresponding indices and create a tensor\n","        model.zero_grad() # Zero out the gradients from previous steps\n","        log_probs = model(context_ids) # Forward pass: get the log probabilities for the target word given the context\n","        label = torch.tensor([word_to_ix[target]], dtype=torch.long) # Create a tensor for the target word's index\n","        loss = loss_function(log_probs, label) # Calculate the loss between the predicted log probabilities and the actual target\n","        loss.backward() # Backward pass: compute gradients of the loss with respect to the model's parameters\n","        optimizer.step() # Update the model's parameters using the optimizer\n","        total_loss += loss.item() # Accumulate the loss for the current epoch\n","    losses.append(total_loss) # Append the total loss for the current epoch to the losses list\n","\n","\n","# Plot the loss values over the epochs using matplotlib\n","\n","plt.plot(range(20), losses)"]},{"cell_type":"markdown","id":"b80ef96b","metadata":{"id":"b80ef96b"},"source":["### 3. Sentence Embedding with SentenceBERT"]},{"cell_type":"code","execution_count":null,"id":"bf22b614","metadata":{"id":"bf22b614"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5ee21465","metadata":{"id":"5ee21465"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b817305a","metadata":{"id":"b817305a"},"source":["#### Want to Know More about word embedding?\n","\n","In this simple exercise, we used a very simple corpus. It only includes one paragraph. Also, we set the embedding dimension to be 3 for easy visualization. The result is not very accurate.\n","\n","Some institutions or scholars have trained word embeddings from billions of documents. Those pre-trained word embeddings can be downloaded online. They also provide different options for the embedding dimension, e.g., 100 or 200. You can check the following link for more information:\n","\n","- [Google Word2Vec](https://code.google.com/archive/p/word2vec/)\n","- [Standford GloVe](https://nlp.stanford.edu/projects/glove/)"]},{"cell_type":"markdown","id":"85b66b9b","metadata":{"id":"85b66b9b"},"source":["### 4. Other application of NLP approaches.\n","\n","We have introduced different NLP methods in this session, some of which can be applied to tasks beyound text processing. Below shows you a pesudo scenario.\n","\n","The classification of occupations (职业) normally follows a top-down manner. For instance, China has defined 8 general occupation types, each further subdivided into more specific categories [[ref]](https://oss.baigongbao.com/2021/07/18/BkANmPhfNr.pdf). However, due to the rapid evolution of society, new occupation types often emerge that do not fit into existing categories. As a research assistant, you have been tasked with classifying these new occupations into the existing categories. To accomplish this, you recall the concept of \"embedding\" from your CSS 5220 class. You decide to create embeddings for each occupation type and use similarity measures to match new occupations to existing categories.\n","\n","There are various methods to achieve this, an intuitive one is to create embeddings based on job advertisement texts. Unfortunately, your advisor does not have access to this type of data. Instead, he provides you with user-based data, consisting of 1,000 job applicants and the jobs each applicant has applied for. Below shows two exemplary records of the data:\n","\n","    1. {'name': '张三', 'jobs_applied': ['数据分析师'，'机器学习工程师'，'产品经理'，'数据运营']}\n","    2. {'name': '李四', 'jobs_applied': ['服务员'，'前台'，'销售助理'，'办公室文员'，'零售店员']}\n","\n","Therefore, the question becomes: how can you create embeddings for occupations based on this available data?\n","\n","<span style='background-color: #FFFF00;'> Please read the above question and document the steps that you will take to solve the question (**2 Points**).</span>\n","\n","(**Hint**: I expect you use Word2Vec embedding)"]},{"cell_type":"markdown","id":"7edcac86","metadata":{"id":"7edcac86"},"source":["---\n","\n","\n","\n","**Answer:** (double click to start typing your answers)\n","\n","\n","----"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}